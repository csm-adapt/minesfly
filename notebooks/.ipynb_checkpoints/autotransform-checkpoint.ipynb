{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary/Overview\n",
    "\n",
    "This script looks for .TXM files, extracts the pixel size information and stores the tomography in an HDF5 file. The structure of this file is guaranteed to contain:\n",
    "\n",
    "```\n",
    "    /tomograph\n",
    "    /tomograph.attrs[\"pixel size\"]\n",
    "```\n",
    "\n",
    "It is stored with the same name as the original .TXM file, but with an .hdf5 file extension. If the .hdf5 file already exists, no conversion is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from config.pythonConsoleAutoImport import *\n",
    "import os, sys\n",
    "orsProgramData = os.path.join(os.environ['ProgramData'], 'ORS', 'Dragonfly30', 'python')\n",
    "sys.path.append(orsProgramData)\n",
    "sys.path.append('C:\\Program Files\\Dragonfly')\n",
    "os.system('registerDLLs.bat')\n",
    "import logging, time\n",
    "from glob import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import h5py\n",
    "from OrsPlugins.orsimageloader import OrsImageLoader\n",
    "from OrsPlugins.orsimagesaver import OrsImageSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format='[%(asctime)s](%(levelname)s) %(message)', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "def info(fmt, *pos):\n",
    "    # logging.info(fmt, *pos)\n",
    "    print('[{}](INFO): '.format(time.asctime(time.gmtime())) + fmt % pos)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def debug(fmt, *pos):\n",
    "    # logging.debug(fmt, *pos)\n",
    "    print('[{}](DEBUG): '.format(time.asctime(time.gmtime())) + fmt % pos)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the directory in which to search for new TXM files\n",
    "searchdir = \"//CGFYZ72/Data_PrimaryWS/inconel\"\n",
    "\n",
    "# glob pattern to search\n",
    "searchstr = \"{}/**/*.txm\".format(searchdir)\n",
    "\n",
    "# Exclude files that match a regular expression. The regular expression\n",
    "# is applied to the search path, not the destination path.\n",
    "exclude = [\n",
    "    re.compile(r'(warmup|warm-up|warm)', re.I) # do not include warmup files.\n",
    "]\n",
    "\n",
    "# specify the parent directory where the converted files will live\n",
    "destparent = \"Z:\\Inconel718\\converted\"\n",
    "\n",
    "# create subfolders from the parent directory:\n",
    "# 0 - all converted files will be written to the parent directory\n",
    "# 1 - the first child directory (whose parent is `searchdir`) will be created\n",
    "# ...\n",
    "# >= depth - the folder hierarchy from searchdir will be fully replicated\n",
    "# -1 - as above, the folder hierarchy from searchdir will be fully replicated\n",
    "destdepth = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "1. Use `glob` to search `searchdir` for all .TXM files $\\rightarrow$ `txm_files`\n",
    "2. From `txm_files`, construct corresponding .hdf5 filenames $\\rightarrow$ `hdf5_files`\n",
    "3. Keep from `txm_files` and `hdf5_files` only those filenames where the filename in `hdf5_files` does *not* exist.\n",
    "4. For each TXM/HDF5 file pair:\n",
    "\n",
    "   1. read file $\\rightarrow$ `channel`\n",
    "   2. extract pixel size $\\rightarrow$ `pixelsize`\n",
    "   3. create numpy array from `channel` $\\rightarrow$ `data`\n",
    "   3. open HDF5 file for writing $\\rightarrow$ `h5`\n",
    "   3. create HDF5 dataset from `data` $\\rightarrow$ `h5[\"tomograph\"]`\n",
    "   5. write `pixelsize` to `h5[\"tomograph\"].attr[\"pixel size\"]`\n",
    "   6. close HDF5 file\n",
    "   7. delete new channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all .txm files from the working directory\n",
    "txm_files = glob(searchstr, recursive=True)\n",
    "for exclusion in exclude:\n",
    "    txm_files = [txm for txm in txm_files if not re.search(exclusion, txm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate HDF5 filenames from TXM filenames.\n",
    "def is_network_path(path):\n",
    "    return path.startswith('//')\n",
    "\n",
    "def split_path(path):\n",
    "    if is_network_path(path):\n",
    "        std_path = path.replace(os.sep, '/')\n",
    "        return std_path.split('/')\n",
    "    else:\n",
    "        return path.split(os.sep)\n",
    "    \n",
    "def join_path(dir_list):\n",
    "    return os.sep.join(dir_list)\n",
    "\n",
    "# skip the path leading up to the parent search directory\n",
    "search_start = len(split_path(searchdir))\n",
    "# include in the destination only the first `destdepth` entries\n",
    "search_end   = search_start + destdepth\n",
    "# Convert the destination path to a list of folders. This makes\n",
    "# construction of the full path much easier\n",
    "split_path_to_destination    = split_path(destparent)\n",
    "\n",
    "hdf5_files = []\n",
    "for txm in txm_files:\n",
    "    # separate the filename from the path\n",
    "    srcpath, ifile = os.path.split(txm)\n",
    "    srcpath = split_path(srcpath)\n",
    "    # construct HDF5 filename\n",
    "    ofile = os.path.splitext(ifile)[0] + '.hdf5'\n",
    "    # construct the list of folders starting with the full destination directory,\n",
    "    # through the `destdepth` child folders from the search directory, finally excluding\n",
    "    # the folders up to the parent folder in the search directory\n",
    "    dstpath = split_path_to_destination + srcpath[search_start:search_end]\n",
    "    # combine this newly created directory with the output filename\n",
    "    ofile = join_path(dstpath + [ofile])\n",
    "    # handle network paths\n",
    "    if is_network_path(destparent):\n",
    "        ofile = '//' + ofile.replace(os.sep, '/')\n",
    "    # append the new filename to the list of hdf5_files\n",
    "    hdf5_files.append(ofile)\n",
    "\n",
    "# DELETE\n",
    "#hdf5_files = [os.path.split(os.path.splitext(txm)[0] + \".hdf5\") for txm in txm_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep only those txm/hdf5 files for which the hdf5 file does *not* already exist\n",
    "exists = [os.path.isfile(h5) for h5 in hdf5_files]\n",
    "txm_files = [txm for txm,complete in zip(txm_files, exists) if not complete]\n",
    "hdf5_files = [hdf for hdf,complete in zip(hdf5_files, exists) if not complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_directory_hierarchy(filename):\n",
    "    dirname = os.path.dirname(filename)\n",
    "    if not os.path.exists(dirname):\n",
    "        try:\n",
    "            os.makedirs(dirname)\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    return dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fri Jun  9 15:48:00 2017](INFO): Input: //CGFYZ72/Data_PrimaryWS/inconel\\P002_B001\\Uncompressed\\P002_B001_C14-0.4X\\P002_B001_C14_2016-04-14_134621\\Inconel-0.4X\\P002_B001_C14-0.4X_recon.txm\n",
      "[Fri Jun  9 15:48:00 2017](INFO): Output: Z:\\Inconel718\\converted\\P002_B001\\P002_B001_C14-0.4X_recon.hdf5\n"
     ]
    }
   ],
   "source": [
    "# process the files\n",
    "for txm,hdf in zip(txm_files, hdf5_files):\n",
    "    info(\"Input: {}\".format(txm))\n",
    "    info(\"Output: {}\".format(hdf))\n",
    "    # read the file using Dragonfly/ORS ImageLoader\n",
    "    try:\n",
    "        channel   = OrsImageLoader.createDatasetDeterminingGeometryFromFiles([txm])\n",
    "        debug(\"TXM read successfully.\")\n",
    "    except:\n",
    "        debug(\"TXM file ({}) failed to read. Skipping.\".format(txm))\n",
    "        continue\n",
    "    # extract the pixel size\n",
    "    pixelsize = channel.getXSpacing()*1e6 # convert pixel size to microns\n",
    "    info(\"Pixel size: %f\", pixelsize)\n",
    "    # get the data as a numpy ndarray\n",
    "    data      = channel.getNDArray()\n",
    "    debug(\"Extracted NDArray\")\n",
    "    # make sure the directory where this file will live actually exists\n",
    "    path = create_directory_hierarchy(hdf)\n",
    "    debug(\"%s available for writing\", path)\n",
    "    # open the file for reading\n",
    "    try:\n",
    "        ofile     = h5py.File(hdf, \"w\")\n",
    "        debug(\"Opened %s for writing.\", hdf)\n",
    "        # create the dataset from the channel data\n",
    "        dset      = ofile.create_dataset(\"tomograph\", data=data)\n",
    "        debug(\"Created HDF5 dataset from TXM data.\")\n",
    "        # add attributes\n",
    "        dset.attrs[\"pixel size\"] = pixelsize\n",
    "        dset.attrs[\"pixel units\"] = r'$\\mu m$'\n",
    "        debug(\"Added pixels size and unit attributes.\")\n",
    "        debug(\"HDF5 written successfully.\")\n",
    "    except:\n",
    "        # the HDF5 file must be closed before it can be removed.\n",
    "        try:\n",
    "            ofile.close()\n",
    "        except:\n",
    "            pass\n",
    "        os.remove(hdf)\n",
    "        debug(\"HDF5 (%s) removed.\", hdf)\n",
    "        debug(\"HDF5 (%s) failed to write. Skipping\")\n",
    "        continue\n",
    "    finally:\n",
    "        try:\n",
    "            ofile.close()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # delete the channel object\n",
    "    channel.deleteObject()\n",
    "    debug(\"Deleted TXM channel object.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Dragonfly",
   "language": "python",
   "name": "dragonfly_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
